COMPANY: CODTECH IT SOLUTIONS

NAME: PRATHAM KUMAR

INTERN ID: CT4MKOP

DOMAIN: DATA SCIENCE

DURATION: 16 WEEKS (4 MONTHS)

MENTOR: NEELA SANTHOSH KUMAR

DESCRIPTION OF TASK:

To analyze a large dataset with Apache Spark using operations like filtering, grouping, and aggregations, you can follow these steps:
Steps for Analysis:
Initialize Spark Session - This allows you to interact with Spark.
Load the Dataset - Load the dataset into a DataFrame (you can use various formats such as CSV, Parquet, etc.).
Filtering - Apply filters to the data based on conditions.
Grouping - Group the data by specific columns.
Aggregations - Apply aggregation functions like sum, count, avg, etc., to grouped data.
Show Results - View the results of your operations.

OUTPUT:
![Image](https://github.com/user-attachments/assets/e4a60828-74b0-4b2c-9206-0233a7063746)
![Image](https://github.com/user-attachments/assets/1a95f345-1a5e-48a9-bfe2-5a72ce9cd3bd)
![Image](https://github.com/user-attachments/assets/ac39a1f7-4c96-4c31-801f-dfa578e13fd4)
![Image](https://github.com/user-attachments/assets/8c5e0d49-a214-4572-89b6-585adf6234ff)
